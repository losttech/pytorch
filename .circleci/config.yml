binary_checkout:
  command: .circleci/scripts/binary_checkout.sh
  name: Checkout pytorch/builder repo
binary_install_miniconda:
  command: .circleci/scripts/binary_install_miniconda.sh
  name: Install miniconda
  no_output_timeout: 1h
binary_linux_build_params:
  docker: &id003
  - image: << parameters.docker_image >>
  environment: &id004
    ANACONDA_USER: pytorch
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    LIBTORCH_VARIANT: << parameters.libtorch_variant >>
  parameters: &id005
    build_environment:
      default: ''
      type: string
    docker_image:
      default: ''
      type: string
    libtorch_variant:
      default: ''
      type: string
    resource_class:
      default: 2xlarge+
      type: string
  resource_class: << parameters.resource_class >>
binary_linux_test_upload_params:
  environment: &id006
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    LIBTORCH_VARIANT: << parameters.libtorch_variant >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
  parameters: &id007
    build_environment:
      default: ''
      type: string
    docker_image:
      default: ''
      type: string
    libtorch_variant:
      default: ''
      type: string
    resource_class:
      default: medium
      type: string
    use_cuda_docker_runtime:
      default: ''
      type: string
  resource_class: << parameters.resource_class >>
binary_mac_params:
  environment: &id008
    BUILD_ENVIRONMENT: << parameters.build_environment >>
  parameters: &id009
    build_environment:
      default: ''
      type: string
binary_populate_env:
  command: .circleci/scripts/binary_populate_env.sh
  name: Set up binary env variables
binary_run_in_docker:
  command: .circleci/scripts/binary_run_in_docker.sh
  name: Run in docker
binary_windows_params:
  environment: &id010
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    BUILD_FOR_SYSTEM: windows
    JOB_EXECUTOR: <<parameters.executor>>
  parameters:
    build_environment:
      default: ''
      type: string
    executor:
      default: windows-xlarge-cpu-with-nvidia-cuda
      type: string
commands:
  brew_install:
    description: Install Homebrew formulae
    parameters:
      formulae:
        default: ''
        type: string
    steps:
    - run:
        command: 'set -ex

          export HOMEBREW_NO_AUTO_UPDATE=1

          brew install << parameters.formulae >>

          '
        name: Install << parameters.formulae >>
        no_output_timeout: 10m
  brew_update:
    description: Update Homebrew and install base formulae
    steps:
    - run:
        command: 'set -ex


          # Update repositories manually.

          # Running `brew update` produces a comparison between the

          # current checkout and the updated checkout, which takes a

          # very long time because the existing checkout is 2y old.

          for path in $(find /usr/local/Homebrew -type d -name .git)

          do

          cd $path/..

          git fetch --depth=1 origin

          git reset --hard origin/master

          done


          export HOMEBREW_NO_AUTO_UPDATE=1


          # Install expect and moreutils so that we can call `unbuffer` and `ts`.

          # moreutils installs a `parallel` executable by default, which conflicts

          # with the executable from the GNU `parallel`, so we must unlink GNU

          # `parallel` first, and relink it afterwards.

          brew unlink parallel

          brew install moreutils

          brew link parallel --overwrite

          brew install expect

          '
        name: Update Homebrew
        no_output_timeout: 10m
  calculate_docker_image_tag:
    description: Calculates the docker image tag
    steps:
    - run:
        command: 'DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)

          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${BASH_ENV}"

          '
        name: Calculate docker image hash
  designate_upload_channel:
    description: inserts the correct upload channel into ${BASH_ENV}
    steps:
    - run:
        command: "our_upload_channel=nightly\n# On tags upload to test instead\nif\
          \ [[ -n \"${CIRCLE_TAG}\" ]]; then\n  our_upload_channel=test\nfi\necho\
          \ \"export UPLOAD_CHANNEL=${our_upload_channel}\" >> ${BASH_ENV}\n"
        name: adding UPLOAD_CHANNEL to BASH_ENV
  optional_merge_target_branch:
    steps:
    - run:
        command: "if [[ -n \"$CIRCLE_PULL_REQUEST\" && \"$CIRCLE_BRANCH\" != \"nightly\"\
          \ ]]; then\n  PR_NUM=$(basename $CIRCLE_PULL_REQUEST)\n  CIRCLE_PR_BASE_BRANCH=$(curl\
          \ -s https://api.github.com/repos/$CIRCLE_PROJECT_USERNAME/$CIRCLE_PROJECT_REPONAME/pulls/$PR_NUM\
          \ | jq -r '.base.ref')\n  if [[ \"${BUILD_ENVIRONMENT}\" == *\"xla\"* ||\
          \ \"${BUILD_ENVIRONMENT}\" == *\"gcc5\"* ]] ; then\n    set -x\n    git\
          \ config --global user.email \"circleci.ossci@gmail.com\"\n    git config\
          \ --global user.name \"CircleCI\"\n    git config remote.origin.url https://github.com/pytorch/pytorch.git\n\
          \    git config --add remote.origin.fetch +refs/heads/master:refs/remotes/origin/master\n\
          \    git fetch --tags --progress https://github.com/pytorch/pytorch.git\
          \ +refs/heads/master:refs/remotes/origin/master --depth=100 --quiet\n  \
          \  # PRs generated from ghstack has format CIRCLE_PR_BASE_BRANCH=gh/xxx/1234/base\n\
          \    if [[ \"${CIRCLE_PR_BASE_BRANCH}\" == \"gh/\"* ]]; then\n      CIRCLE_PR_BASE_BRANCH=master\n\
          \    fi\n    export GIT_MERGE_TARGET=`git log -n 1 --pretty=format:\"%H\"\
          \ origin/$CIRCLE_PR_BASE_BRANCH`\n    echo \"GIT_MERGE_TARGET: \" ${GIT_MERGE_TARGET}\n\
          \    export GIT_COMMIT=${CIRCLE_SHA1}\n    echo \"GIT_COMMIT: \" ${GIT_COMMIT}\n\
          \    git checkout -f ${GIT_COMMIT}\n    git reset --hard ${GIT_COMMIT}\n\
          \    git merge --allow-unrelated-histories --no-edit --no-ff ${GIT_MERGE_TARGET}\n\
          \    echo \"Merged $CIRCLE_PR_BASE_BRANCH branch before building in environment\
          \ $BUILD_ENVIRONMENT\"\n    set +x\n  else\n    echo \"No need to merge\
          \ with $CIRCLE_PR_BASE_BRANCH, skipping...\"\n  fi\nelse\n  echo \"This\
          \ is not a pull request, skipping...\"\nfi\n"
        name: (Optional) Merge target branch
        no_output_timeout: 10m
  run_brew_for_ios_build:
    steps:
    - brew_update
    - brew_install:
        formulae: libtool
  run_brew_for_macos_build:
    steps:
    - brew_update
    - brew_install:
        formulae: libomp
  setup_ci_environment:
    steps:
    - run:
        command: .circleci/scripts/setup_ci_environment.sh
        name: Set Up CI Environment After attach_workspace
        no_output_timeout: 1h
  setup_linux_system_environment:
    steps:
    - run:
        command: .circleci/scripts/setup_linux_system_environment.sh
        name: Set Up System Environment
        no_output_timeout: 1h
  upload_binary_size_for_android_build:
    description: Upload binary size data for Android build
    parameters:
      artifacts:
        default: ''
        type: string
      build_type:
        default: ''
        type: string
    steps:
    - run:
        command: "retry () {\n  $* || (sleep 1 && $*) || (sleep 2 && $*) || (sleep\
          \ 4 && $*) || (sleep 8 && $*)\n}\nretry pip3 install requests\n"
        name: Binary Size - Install Dependencies
        no_output_timeout: 5m
    - run:
        command: "# The artifact file is created inside docker container, which contains\
          \ the result binaries.\n# Now unpackage it into the project folder. The\
          \ subsequent script will scan project folder\n# to locate result binaries\
          \ and report their sizes.\n# If artifact file is not provided it assumes\
          \ that the project folder has been mounted in\n# the docker during build\
          \ and already contains the result binaries, so this step can be skipped.\n\
          export ARTIFACTS=\"<< parameters.artifacts >>\"\nif [ -n \"${ARTIFACTS}\"\
          \ ]; then\n  tar xf \"${ARTIFACTS}\" -C ~/project\nfi\n"
        name: Binary Size - Untar Artifacts
        no_output_timeout: 5m
    - run:
        command: 'cd ~/project

          export ANDROID_BUILD_TYPE="<< parameters.build_type >>"

          export COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)

          python3 .circleci/scripts/upload_binary_size_to_scuba.py android

          '
        name: Binary Size - Upload << parameters.build_type >>
        no_output_timeout: 5m
executors:
  windows-medium-cpu-with-nvidia-cuda:
    machine:
      image: windows-server-2019-vs2019:stable
      resource_class: windows.medium
      shell: bash.exe
  windows-with-nvidia-gpu:
    machine:
      image: windows-server-2019-nvidia:previous
      resource_class: windows.gpu.nvidia.medium
      shell: bash.exe
  windows-xlarge-cpu-with-nvidia-cuda:
    machine:
      image: windows-server-2019-vs2019:stable
      resource_class: windows.xlarge
      shell: bash.exe
jobs:
  anaconda_prune:
    docker:
    - image: continuumio/miniconda3
    environment:
    - PACKAGES: << parameters.packages >>
    - CHANNEL: << parameters.channel >>
    parameters:
      channel:
        default: pytorch-nightly
        description: What channel are we pruning? (eq. pytorch-nightly)
        type: string
      packages:
        default: pytorch
        description: What packages are we pruning? (quoted, space-separated string.
          eg. 'pytorch', 'torchvision torchaudio', etc.)
        type: string
    steps:
    - checkout
    - run:
        command: 'conda install -yq anaconda-client

          '
        name: Install dependencies
        no_output_timeout: 1h
    - run:
        command: 'ANACONDA_API_TOKEN="${CONDA_PYTORCHBOT_TOKEN}" \

          scripts/release/anaconda-prune/run.sh

          '
        name: Prune packages
        no_output_timeout: 1h
  binary_ios_build:
    environment: &id001
      BUILD_ENVIRONMENT: << parameters.build_environment >>
      BUILD_LITE_INTERPRETER: << parameters.lite_interpreter >>
      IOS_ARCH: << parameters.ios_arch >>
      IOS_PLATFORM: << parameters.ios_platform >>
      SELECTED_OP_LIST: << parameters.op_list >>
      USE_PYTORCH_METAL: << parameters.use_metal >>
    macos:
      xcode: '12.0'
    parameters: &id002
      build_environment:
        default: ''
        type: string
      ios_arch:
        default: ''
        type: string
      ios_platform:
        default: ''
        type: string
      lite_interpreter:
        default: '1'
        type: string
      op_list:
        default: ''
        type: string
      use_metal:
        default: '0'
        type: string
    steps:
    - attach_workspace:
        at: ~/workspace
    - checkout
    - run_brew_for_ios_build
    - run:
        command: 'script="/Users/distiller/project/.circleci/scripts/binary_ios_build.sh"

          cat "$script"

          source "$script"

          '
        name: Build
        no_output_timeout: 1h
    - run:
        command: 'script="/Users/distiller/project/.circleci/scripts/binary_ios_test.sh"

          cat "$script"

          source "$script"

          '
        name: Test
        no_output_timeout: 30m
    - persist_to_workspace:
        paths: ios
        root: /Users/distiller/workspace/
  binary_ios_upload:
    environment: *id001
    macos:
      xcode: '12.0'
    parameters: *id002
    steps:
    - attach_workspace:
        at: ~/workspace
    - checkout
    - run_brew_for_ios_build
    - run:
        command: 'script="/Users/distiller/project/.circleci/scripts/binary_ios_upload.sh"

          cat "$script"

          source "$script"

          '
        name: Upload
        no_output_timeout: 1h
  binary_linux_build:
    docker: *id003
    environment: *id004
    parameters: *id005
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - calculate_docker_image_tag
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - run:
        command: "source \"/pytorch/.circleci/scripts/binary_linux_build.sh\"\n# Preserve\
          \ build log\nif [ -f /pytorch/build/.ninja_log ]; then\n  cp /pytorch/build/.ninja_log\
          \ /final_pkgs\nfi\n"
        name: Build
        no_output_timeout: 1h
    - run:
        command: 'ls -lah /final_pkgs

          '
        name: Output binary sizes
        no_output_timeout: 1m
    - run:
        command: 'source /env

          cd /pytorch && export COMMIT_TIME=$(git log --max-count=1 --format=%ct ||
          echo 0)

          python3 -mpip install requests && \

          SCRIBE_GRAPHQL_ACCESS_TOKEN=${SCRIBE_GRAPHQL_ACCESS_TOKEN} \

          python3 /pytorch/.circleci/scripts/upload_binary_size_to_scuba.py || exit
          0

          '
        name: upload build & binary data
        no_output_timeout: 5m
    - persist_to_workspace:
        paths: final_pkgs
        root: /
    - store_artifacts:
        path: /final_pkgs
  binary_linux_test:
    environment: *id006
    machine:
      image: ubuntu-2004:202104-01
    parameters: *id007
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - attach_workspace:
        at: /home/circleci/project
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - run:
        command: .circleci/scripts/binary_linux_test.sh
        name: Prepare test code
        no_output_timeout: 1h
    - run:
        command: .circleci/scripts/binary_run_in_docker.sh
        name: Run in docker
  binary_mac_build:
    environment: *id008
    macos:
      xcode: '12.0'
    parameters: *id009
    steps:
    - checkout
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - brew_update
    - run:
        command: .circleci/scripts/binary_install_miniconda.sh
        name: Install miniconda
        no_output_timeout: 1h
    - run:
        command: '# Do not set -u here; there is some problem with CircleCI

          # variable expansion with PROMPT_COMMAND

          set -ex -o pipefail

          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_build.sh"

          cat "$script"

          source "$script"

          '
        name: Build
        no_output_timeout: 90m
    - run:
        command: '# Do not set -u here; there is some problem with CircleCI

          # variable expansion with PROMPT_COMMAND

          set -ex -o pipefail

          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_test.sh"

          cat "$script"

          source "$script"

          '
        name: Test
        no_output_timeout: 1h
    - persist_to_workspace:
        paths: final_pkgs
        root: /Users/distiller/project
    - store_artifacts:
        path: /Users/distiller/project/final_pkgs
  binary_macos_arm64_build:
    environment: *id008
    macos:
      xcode: 12.3.0
    parameters: *id009
    steps:
    - checkout
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - brew_update
    - run:
        command: .circleci/scripts/binary_install_miniconda.sh
        name: Install miniconda
        no_output_timeout: 1h
    - run:
        command: '# Do not set -u here; there is some problem with CircleCI

          # variable expansion with PROMPT_COMMAND

          set -ex -o pipefail

          export CROSS_COMPILE_ARM64=1

          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_build.sh"

          cat "$script"

          source "$script"

          '
        name: Build
        no_output_timeout: 90m
    - persist_to_workspace:
        paths: final_pkgs
        root: /Users/distiller/project
    - store_artifacts:
        path: /Users/distiller/project/final_pkgs
  binary_upload:
    docker:
    - image: continuumio/miniconda3
    environment:
    - DRY_RUN: disabled
    - PACKAGE_TYPE: << parameters.package_type >>
    - UPLOAD_SUBFOLDER: << parameters.upload_subfolder >>
    parameters:
      package_type:
        default: wheel
        description: What type of package we are uploading (eg. wheel, libtorch, conda)
        type: string
      upload_subfolder:
        default: cpu
        description: What subfolder to put our package into (eg. cpu, cudaX.Y, etc.)
        type: string
    steps:
    - attach_workspace:
        at: /tmp/workspace
    - checkout
    - designate_upload_channel
    - run:
        command: 'conda install -yq anaconda-client

          pip install -q awscli

          '
        name: Install dependencies
        no_output_timeout: 1h
    - run:
        command: "AWS_ACCESS_KEY_ID=\"${PYTORCH_BINARY_AWS_ACCESS_KEY_ID}\" \\\n \
          \ AWS_SECRET_ACCESS_KEY=\"${PYTORCH_BINARY_AWS_SECRET_ACCESS_KEY}\" \\\n\
          \  ANACONDA_API_TOKEN=\"${CONDA_PYTORCHBOT_TOKEN}\" \\\n  .circleci/scripts/binary_upload.sh\n"
        name: Do upload
        no_output_timeout: 1h
  binary_windows_build:
    environment: *id010
    executor: <<parameters.executor>>
    parameters:
      build_environment:
        default: ''
        type: string
      executor:
        default: windows-xlarge-cpu-with-nvidia-cuda
        type: string
    steps:
    - checkout
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - run:
        command: 'set -eux -o pipefail

          script="/c/w/p/.circleci/scripts/binary_windows_build.sh"

          cat "$script"

          source "$script"

          '
        name: Build
        no_output_timeout: 1h
    - persist_to_workspace:
        paths: final_pkgs
        root: C:/w
    - store_artifacts:
        path: C:/w/final_pkgs
  binary_windows_test:
    environment: *id010
    executor: <<parameters.executor>>
    parameters:
      build_environment:
        default: ''
        type: string
      executor:
        default: windows-medium-cpu-with-nvidia-cuda
        type: string
    steps:
    - checkout
    - attach_workspace:
        at: c:/users/circleci/project
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - run:
        command: 'set -eux -o pipefail

          script="/c/w/p/.circleci/scripts/binary_windows_test.sh"

          cat "$script"

          source "$script"

          '
        name: Test
        no_output_timeout: 1h
  docker_build_job:
    environment:
      DOCKER_BUILDKIT: 1
      DOCKER_CLI_EXPERIMENTAL: enabled
      IMAGE_NAME: << parameters.image_name >>
    machine:
      image: ubuntu-2004:202104-01
    parameters:
      image_name:
        default: ''
        type: string
    resource_class: large
    steps:
    - checkout
    - calculate_docker_image_tag
    - run:
        command: "set +x\nexport AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}\n\
          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}\n\
          export AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4\
          \ -d\\\")\nexport AWS_REGION=us-east-1\naws ecr get-login-password --region\
          \ $AWS_REGION|docker login --username AWS \\\n         --password-stdin\
          \ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\nset -x\n# Check if\
          \ image already exists, if it does then skip building it\nif docker manifest\
          \ inspect \"308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/${IMAGE_NAME}:${DOCKER_TAG}\"\
          ; then\n  circleci-agent step halt\n  # circleci-agent step halt doesn't\
          \ actually halt the step so we need to\n  # explicitly exit the step here\
          \ ourselves before it causes too much trouble\n  exit 0\nfi\n# Covers the\
          \ case where a previous tag doesn't exist for the tree\n# this is only really\
          \ applicable on trees that don't have `.circleci/docker` at its merge base,\
          \ i.e. nightly\nif ! git rev-parse \"$(git merge-base HEAD << pipeline.git.base_revision\
          \ >>):.circleci/docker\"; then\n  echo \"Directory '.circleci/docker' not\
          \ found in tree << pipeline.git.base_revision >>, you should probably rebase\
          \ onto a more recent commit\"\n  exit 1\nfi\nPREVIOUS_DOCKER_TAG=$(git rev-parse\
          \ \"$(git merge-base HEAD << pipeline.git.base_revision >>):.circleci/docker\"\
          )\n# If no image exists but the hash is the same as the previous hash then\
          \ we should error out here\nif [[ \"${PREVIOUS_DOCKER_TAG}\" = \"${DOCKER_TAG}\"\
          \ ]]; then\n  echo \"ERROR: Something has gone wrong and the previous image\
          \ isn't available for the merge-base of your branch\"\n  echo \"       contact\
          \ the PyTorch team to restore the original images\"\n  exit 1\nfi\n"
        name: Check if image should be built
    - run:
        command: 'set +x

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}

          set -x

          cd .circleci/docker && ./build_docker.sh

          '
        name: build_docker_image_<< parameters.image_name >>
        no_output_timeout: 1h
  docker_for_ecr_gc_build_job:
    machine:
      image: ubuntu-2004:202104-01
    steps:
    - checkout
    - run:
        command: "cd .circleci/ecr_gc_docker\ndocker build . -t 308535385114.dkr.ecr.us-east-1.amazonaws.com/gc/ecr\n\
          set +x\nexport AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}\n\
          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}\n\
          export AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4\
          \ -d\\\")\nexport AWS_REGION=us-east-1\naws ecr get-login-password --region\
          \ $AWS_REGION|docker login --username AWS \\\n         --password-stdin\
          \ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\nset -x\ndocker push\
          \ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/gc/ecr\n"
        name: build_docker_image_for_ecr_gc
        no_output_timeout: 1h
  ecr_gc_job:
    docker:
    - aws_auth:
        aws_access_key_id: ${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}
        aws_secret_access_key: ${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}
      image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/gc/ecr
    environment:
      IMAGE_TAG: << parameters.tags_to_keep >>
      PROJECT: << parameters.project >>
    parameters:
      project:
        default: pytorch
        type: string
      tags_to_keep:
        type: string
    steps:
    - checkout
    - run:
        command: "GENERATED_IMAGE_TAG=$(\\\n  git log --oneline --pretty='%H' .circleci/docker\
          \ \\\n    | xargs -I '{}' git rev-parse '{}:.circleci/docker' \\\n    |\
          \ paste -sd \",\" -)\necho \"export GENERATED_IMAGE_TAG='${GENERATED_IMAGE_TAG}'\"\
          \ >> ${BASH_ENV}\n"
        name: dynamically generate tags to keep
        no_output_timeout: 1h
    - run:
        command: 'set +x

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}

          set -x

          /usr/bin/gc.py --filter-prefix ${PROJECT}  --ignore-tags "${IMAGE_TAG},${GENERATED_IMAGE_TAG}"

          '
        name: garbage collecting for ecr images
        no_output_timeout: 1h
  promote_conda:
    docker: &id011
    - image: pytorch/release
    environment: &id012
      ANACONDA_API_TOKEN: ${CONDA_PYTORCHBOT_TOKEN}
      AWS_ACCESS_KEY_ID: ${PYTORCH_BINARY_AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${PYTORCH_BINARY_AWS_SECRET_ACCESS_KEY}
      PACKAGE_NAME: << parameters.package_name >>
    parameters: &id013
      package_name:
        default: ''
        description: package name to promote
        type: string
    steps:
    - checkout
    - run:
        command: 'scripts/release/promote/conda_to_conda.sh

          '
        name: Running promote script
  promote_s3:
    docker: *id011
    environment: *id012
    parameters: *id013
    steps:
    - checkout
    - run:
        command: 'scripts/release/promote/wheel_to_s3.sh

          '
        name: Running promote script
  pytorch_android_gradle_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build
      DOCKER_IMAGE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c
      PYTHON_VERSION: '3.6'
    machine:
      image: ubuntu-2004:202104-01
    resource_class: large
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: 'set -eux

          docker_image_commit=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}


          docker_image_libtorch_android_x86_32=${docker_image_commit}-android-x86_32

          docker_image_libtorch_android_x86_64=${docker_image_commit}-android-x86_64

          docker_image_libtorch_android_arm_v7a=${docker_image_commit}-android-arm-v7a

          docker_image_libtorch_android_arm_v8a=${docker_image_commit}-android-arm-v8a


          echo "docker_image_commit: "${docker_image_commit}

          echo "docker_image_libtorch_android_x86_32: "${docker_image_libtorch_android_x86_32}

          echo "docker_image_libtorch_android_x86_64: "${docker_image_libtorch_android_x86_64}

          echo "docker_image_libtorch_android_arm_v7a: "${docker_image_libtorch_android_arm_v7a}

          echo "docker_image_libtorch_android_arm_v8a: "${docker_image_libtorch_android_arm_v8a}


          # x86_32

          time docker pull ${docker_image_libtorch_android_x86_32} >/dev/null

          export id_x86_32=$(docker run --env-file "${BASH_ENV}" -e GRADLE_OFFLINE=1
          --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins
          ${docker_image_libtorch_android_x86_32})


          export COMMAND=''((echo "sudo chown -R jenkins workspace") | docker exec
          -u jenkins -i "$id_x86_32" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          # arm-v7a

          time docker pull ${docker_image_libtorch_android_arm_v7a} >/dev/null

          export id_arm_v7a=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE
          --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_arm_v7a})


          export COMMAND=''((echo "sudo chown -R jenkins workspace") | docker exec
          -u jenkins -i "$id_arm_v7a" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          mkdir -p ~/workspace/build_android_install_arm_v7a

          docker cp $id_arm_v7a:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_arm_v7a


          # x86_64

          time docker pull ${docker_image_libtorch_android_x86_64} >/dev/null

          export id_x86_64=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE
          --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_64})


          export COMMAND=''((echo "sudo chown -R jenkins workspace") | docker exec
          -u jenkins -i "$id_x86_64" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          mkdir -p ~/workspace/build_android_install_x86_64

          docker cp $id_x86_64:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_x86_64


          # arm-v8a

          time docker pull ${docker_image_libtorch_android_arm_v8a} >/dev/null

          export id_arm_v8a=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE
          --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_arm_v8a})


          export COMMAND=''((echo "sudo chown -R jenkins workspace") | docker exec
          -u jenkins -i "$id_arm_v8a" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          mkdir -p ~/workspace/build_android_install_arm_v8a

          docker cp $id_arm_v8a:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_arm_v8a


          docker cp ~/workspace/build_android_install_arm_v7a $id_x86_32:/var/lib/jenkins/workspace/build_android_install_arm_v7a

          docker cp ~/workspace/build_android_install_x86_64 $id_x86_32:/var/lib/jenkins/workspace/build_android_install_x86_64

          docker cp ~/workspace/build_android_install_arm_v8a $id_x86_32:/var/lib/jenkins/workspace/build_android_install_arm_v8a


          # run gradle buildRelease

          export COMMAND=''((echo "sudo chown -R jenkins workspace && cd workspace
          && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins
          -i "$id_x86_32" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          mkdir -p ~/workspace/build_android_artifacts

          docker cp $id_x86_32:/var/lib/jenkins/workspace/android/artifacts.tgz ~/workspace/build_android_artifacts/


          output_image=$docker_image_libtorch_android_x86_32-gradle

          docker commit "$id_x86_32" ${output_image}

          time docker push ${output_image}

          '
        name: pytorch android gradle build
        no_output_timeout: 1h
    - upload_binary_size_for_android_build:
        artifacts: /home/circleci/workspace/build_android_artifacts/artifacts.tgz
        build_type: prebuilt
    - store_artifacts:
        destination: artifacts.tgz
        path: ~/workspace/build_android_artifacts/artifacts.tgz
  pytorch_android_gradle_build-x86_32:
    environment:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build-only-x86_32
      DOCKER_IMAGE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c
      PYTHON_VERSION: '3.6'
    machine:
      image: ubuntu-2004:202104-01
    resource_class: large
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - checkout
    - setup_ci_environment
    - run:
        command: 'set -e

          docker_image_libtorch_android_x86_32=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}-android-x86_32

          echo "docker_image_libtorch_android_x86_32: "${docker_image_libtorch_android_x86_32}


          # x86

          time docker pull ${docker_image_libtorch_android_x86_32} >/dev/null

          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt
          seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_32})


          export COMMAND=''((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}"
          && echo "export GRADLE_OFFLINE=1" && echo "sudo chown -R jenkins workspace
          && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker
          exec -u jenkins -i "$id" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          mkdir -p ~/workspace/build_android_x86_32_artifacts

          docker cp $id:/var/lib/jenkins/workspace/android/artifacts.tgz ~/workspace/build_android_x86_32_artifacts/


          output_image=${docker_image_libtorch_android_x86_32}-gradle

          docker commit "$id" ${output_image}

          time docker push ${output_image}

          '
        name: pytorch android gradle build only x86_32 (for PR)
        no_output_timeout: 1h
    - upload_binary_size_for_android_build:
        artifacts: /home/circleci/workspace/build_android_x86_32_artifacts/artifacts.tgz
        build_type: prebuilt-single
    - store_artifacts:
        destination: artifacts.tgz
        path: ~/workspace/build_android_x86_32_artifacts/artifacts.tgz
  pytorch_android_gradle_custom_build_single:
    environment: &id017
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single
      BUILD_LITE_INTERPRETER: << parameters.lite_interpreter >>
      DOCKER_IMAGE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c
      PYTHON_VERSION: '3.6'
      SELECTED_OP_LIST: << parameters.op_list >>
    machine:
      image: ubuntu-2004:202104-01
    parameters: &id018
      build_environment:
        default: ''
        type: string
      lite_interpreter:
        default: '1'
        type: string
      op_list:
        default: ''
        type: string
    resource_class: large
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - checkout
    - calculate_docker_image_tag
    - setup_ci_environment
    - run:
        command: 'set -e

          # Unlike other gradle jobs, it''s not worth building libtorch in a separate
          CI job and share via docker, because:

          # 1) Not shareable: it''s custom selective build, which is different from
          default libtorch mobile build;

          # 2) Not parallelizable by architecture: it only builds libtorch for one
          architecture;


          echo "DOCKER_IMAGE: ${DOCKER_IMAGE}:${DOCKER_TAG}"

          time docker pull ${DOCKER_IMAGE}:${DOCKER_TAG} >/dev/null


          git submodule sync && git submodule update -q --init --recursive --depth
          1

          VOLUME_MOUNTS="-v /home/circleci/project/:/var/lib/jenkins/workspace"

          export id=$(docker run --env-file "${BASH_ENV}" ${VOLUME_MOUNTS} --cap-add=SYS_PTRACE
          --security-opt seccomp=unconfined --cap-add=SYS_PTRACE --security-opt seccomp=unconfined
          -t -d -w /var/lib/jenkins ${DOCKER_IMAGE}:${DOCKER_TAG})


          export COMMAND=''((echo "export GRADLE_OFFLINE=1" && echo "export BUILD_LITE_INTERPRETER=${BUILD_LITE_INTERPRETER}"
          && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh")
          | docker exec -u jenkins -i "$id" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          # Skip docker push as this job is purely for size analysis purpose.

          # Result binaries are already in `/home/circleci/project/` as it''s mounted
          instead of copied.

          '
        name: pytorch android gradle custom build single architecture (for PR)
        no_output_timeout: 1h
    - upload_binary_size_for_android_build:
        build_type: custom-build-single
  pytorch_android_publish_snapshot:
    environment:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-publish-snapshot
      DOCKER_IMAGE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c
      PYTHON_VERSION: '3.6'
    machine:
      image: ubuntu-2004:202104-01
    resource_class: large
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: 'set -eux

          docker_image_commit=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}


          docker_image_libtorch_android_x86_32_gradle=${docker_image_commit}-android-x86_32-gradle


          echo "docker_image_commit: "${docker_image_commit}

          echo "docker_image_libtorch_android_x86_32_gradle: "${docker_image_libtorch_android_x86_32_gradle}


          # x86_32

          time docker pull ${docker_image_libtorch_android_x86_32_gradle} >/dev/null

          export id_x86_32=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE
          --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_32_gradle})


          export COMMAND=''((echo "sudo chown -R jenkins workspace" && echo "export
          BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export SONATYPE_NEXUS_USERNAME=${SONATYPE_NEXUS_USERNAME}"
          && echo "export SONATYPE_NEXUS_PASSWORD=${SONATYPE_NEXUS_PASSWORD}" && echo
          "export ANDROID_SIGN_KEY=${ANDROID_SIGN_KEY}" && echo "export ANDROID_SIGN_PASS=${ANDROID_SIGN_PASS}"
          && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/publish_android_snapshot.sh")
          | docker exec -u jenkins -i "$id_x86_32" bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          output_image=${docker_image_libtorch_android_x86_32_gradle}-publish-snapshot

          docker commit "$id_x86_32" ${output_image}

          time docker push ${output_image}

          '
        name: pytorch android gradle build
        no_output_timeout: 1h
  pytorch_cpp_doc_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-cpp-doc-push
      DOCKER_IMAGE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
    machine:
      image: ubuntu-2004:202104-01
    resource_class: large
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: 'set -ex

          export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}

          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}

          tag=${CIRCLE_TAG:1:5}

          target=${tag:-master}

          echo "building for ${target}"

          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null

          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt
          seccomp=unconfined -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})


          export COMMAND=''((echo "sudo chown -R jenkins workspace && cd workspace
          && ''"export CIRCLE_SHA1=''$CIRCLE_SHA1''"'' && . ./.circleci/scripts/cpp_doc_push_script.sh
          docs/"$target" master") | docker exec -u jenkins -i "$id" bash) 2>&1''


          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          mkdir -p ~/workspace/build_artifacts

          docker cp $id:/var/lib/jenkins/workspace/cppdocs/ /tmp/workspace


          # Save the docs build so we can debug any problems

          export DEBUG_COMMIT_DOCKER_IMAGE=${COMMIT_DOCKER_IMAGE}-debug

          docker commit "$id" ${DEBUG_COMMIT_DOCKER_IMAGE}

          time docker push ${DEBUG_COMMIT_DOCKER_IMAGE}

          '
        name: Doc Build and Push
        no_output_timeout: 1h
    - persist_to_workspace:
        paths:
        - .
        root: /tmp/workspace
  pytorch_doc_push:
    machine:
      image: ubuntu-2004:202104-01
    parameters:
      branch:
        default: master
        type: string
    resource_class: medium
    steps:
    - attach_workspace:
        at: /tmp/workspace
    - run:
        command: "# set credentials for https pushing\ncat > ~/.netrc \\<<DONE\n \
          \ machine github.com\n  login pytorchbot\n  password ${GITHUB_PYTORCHBOT_TOKEN}\n\
          DONE\n"
        name: Generate netrc
    - run:
        command: 'pushd /tmp/workspace

          git push -u origin "<< parameters.branch >>"

          '
        name: Docs push
  pytorch_doc_test:
    environment:
      BUILD_ENVIRONMENT: pytorch-doc-test
      DOCKER_IMAGE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
    machine:
      image: ubuntu-2004:202104-01
    resource_class: medium
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: 'set -ex

          export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}

          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}

          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null

          export id=$(docker run --cap-add=SYS_PTRACE --security-opt seccomp=unconfined
          -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})

          export COMMAND=''((echo "sudo chown -R jenkins workspace && cd workspace
          && . ./.jenkins/pytorch/docs-test.sh") | docker exec -u jenkins -i "$id"
          bash) 2>&1''

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          '
        name: Doc test
        no_output_timeout: 30m
  pytorch_ios_build:
    environment: *id001
    macos:
      xcode: '12.0'
    parameters: *id002
    steps:
    - checkout
    - run_brew_for_ios_build
    - run:
        command: 'set -e

          PROJ_ROOT=/Users/distiller/project

          cd ${PROJ_ROOT}/ios/TestApp

          # install fastlane

          sudo gem install bundler && bundle install

          # install certificates

          echo ${IOS_CERT_KEY} >> cert.txt

          base64 --decode cert.txt -o Certificates.p12

          rm cert.txt

          bundle exec fastlane install_cert

          # install the provisioning profile

          PROFILE=PyTorch_CI_2021.mobileprovision

          PROVISIONING_PROFILES=~/Library/MobileDevice/Provisioning\ Profiles

          mkdir -pv "${PROVISIONING_PROFILES}"

          cd "${PROVISIONING_PROFILES}"

          echo ${IOS_SIGN_KEY} >> cert.txt

          base64 --decode cert.txt -o ${PROFILE}

          rm cert.txt

          '
        name: Run Fastlane
        no_output_timeout: 1h
    - run:
        command: "set -e\nexport IN_CI=1\nWORKSPACE=/Users/distiller/workspace\nPROJ_ROOT=/Users/distiller/project\n\
          export TCLLIBPATH=\"/usr/local/lib\"\n\n# Install conda\ncurl --retry 3\
          \ -o ~/conda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n\
          chmod +x ~/conda.sh\n/bin/bash ~/conda.sh -b -p ~/anaconda\nexport PATH=\"\
          ~/anaconda/bin:${PATH}\"\nsource ~/anaconda/bin/activate\n\n# Install dependencies\n\
          retry () {\n    $*  || (sleep 1 && $*) || (sleep 2 && $*) || (sleep 4 &&\
          \ $*) || (sleep 8 && $*)\n}\n\nretry conda install numpy ninja pyyaml mkl\
          \ mkl-include setuptools cmake cffi requests typing_extensions --yes\n\n\
          # sync submodules\ncd ${PROJ_ROOT}\ngit submodule sync\ngit submodule update\
          \ --init --recursive --depth 1\n\n# export\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"\
          $(dirname $(which conda))/../\"}\n\n# run build script\nchmod a+x ${PROJ_ROOT}/scripts/build_ios.sh\n\
          echo \"IOS_ARCH: ${IOS_ARCH}\"\necho \"IOS_PLATFORM: ${IOS_PLATFORM}\"\n\
          echo \"USE_PYTORCH_METAL\": \"${USE_METAL}\"\necho \"BUILD_LITE_INTERPRETER\"\
          : \"${BUILD_LITE_INTERPRETER}\"\n\n#check the custom build flag\necho \"\
          SELECTED_OP_LIST: ${SELECTED_OP_LIST}\"\nif [ -n \"${SELECTED_OP_LIST}\"\
          \ ]; then\n    export SELECTED_OP_LIST=\"${PROJ_ROOT}/ios/TestApp/custom_build/${SELECTED_OP_LIST}\"\
          \nfi\nexport IOS_ARCH=${IOS_ARCH}\nexport IOS_PLATFORM=${IOS_PLATFORM}\n\
          if [ ${IOS_PLATFORM} != \"SIMULATOR\" ]; then\n  export USE_PYTORCH_METAL=${USE_METAL}\n\
          fi\nunbuffer ${PROJ_ROOT}/scripts/build_ios.sh 2>&1 | ts\n"
        name: Build
        no_output_timeout: 1h
    - run:
        command: "set -e\nif [ ${BUILD_LITE_INTERPRETER} == 0 ]; then\n  echo \"Run\
          \ Build Test is not for full jit, skipping.\"\n  exit 0\nfi\nPROJ_ROOT=/Users/distiller/project\n\
          PROFILE=PyTorch_CI_2021\n# run the ruby build script\nif ! [ -x \"$(command\
          \ -v xcodebuild)\" ]; then\n  echo 'Error: xcodebuild is not installed.'\n\
          \  exit 1\nfi\necho ${IOS_DEV_TEAM_ID}\nif [ ${IOS_PLATFORM} != \"SIMULATOR\"\
          \ ]; then\n  ruby ${PROJ_ROOT}/scripts/xcode_build.rb -i ${PROJ_ROOT}/build_ios/install\
          \ -x ${PROJ_ROOT}/ios/TestApp/TestApp.xcodeproj -p ${IOS_PLATFORM} -c ${PROFILE}\
          \ -t ${IOS_DEV_TEAM_ID}\nelse\n  ruby ${PROJ_ROOT}/scripts/xcode_build.rb\
          \ -i ${PROJ_ROOT}/build_ios/install -x ${PROJ_ROOT}/ios/TestApp/TestApp.xcodeproj\
          \ -p ${IOS_PLATFORM}\nfi\nif ! [ \"$?\" -eq \"0\" ]; then\n  echo 'xcodebuild\
          \ failed!'\n  exit 1\nfi\n"
        name: Run Build Test
        no_output_timeout: 30m
    - run:
        command: "set -e\nif [ ${IOS_PLATFORM} != \"SIMULATOR\" ]; then\n  echo \"\
          not SIMULATOR build, skip it.\"\n  exit 0\nelif [ ${BUILD_LITE_INTERPRETER}\
          \ == 0 ]; then\n  echo \"Run Simulator Tests is not for full jit, skipping.\"\
          \n  exit 0\nfi\nWORKSPACE=/Users/distiller/workspace\nPROJ_ROOT=/Users/distiller/project\n\
          source ~/anaconda/bin/activate\npip install torch torchvision --progress-bar\
          \ off\n#run unit test\ncd ${PROJ_ROOT}/ios/TestApp/benchmark\npython trace_model.py\n\
          ruby setup.rb\ncd ${PROJ_ROOT}/ios/TestApp\ninstruments -s -devices\nfastlane\
          \ scan\n"
        name: Run Simulator Tests
        no_output_timeout: 2h
  pytorch_linux_bazel_build:
    environment: &id014
      BUILD_ENVIRONMENT: << parameters.build_environment >>
      BUILD_ONLY: << parameters.build_only >>
      DOCKER_IMAGE: << parameters.docker_image >>
      USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
    machine:
      image: ubuntu-2004:202104-01
    parameters: &id015
      build_environment:
        default: ''
        type: string
      build_only:
        default: ''
        type: string
      docker_image:
        default: ''
        type: string
      resource_class:
        default: large
        type: string
      use_cuda_docker_runtime:
        default: ''
        type: string
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: "set -e\n# Pull Docker image and run build\necho \"DOCKER_IMAGE:\
          \ \"${DOCKER_IMAGE}:${DOCKER_TAG}\ntime docker pull ${DOCKER_IMAGE}:${DOCKER_TAG}\
          \ >/dev/null\nexport id=$(docker run --env-file \"${BASH_ENV}\" --cap-add=SYS_PTRACE\
          \ --security-opt seccomp=unconfined --cap-add=SYS_PTRACE --security-opt\
          \ seccomp=unconfined -t -d -w /var/lib/jenkins ${DOCKER_IMAGE}:${DOCKER_TAG})\n\
          \necho \"Do NOT merge master branch into $CIRCLE_BRANCH in environment $BUILD_ENVIRONMENT\"\
          \n\ngit submodule sync && git submodule update -q --init --recursive --depth\
          \ 1\n\ndocker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace\n\
          \nexport COMMAND='((echo \"sudo chown -R jenkins workspace && cd workspace\
          \ && .jenkins/pytorch/build.sh\") | docker exec -u jenkins -i \"$id\" bash)\
          \ 2>&1'\n\necho ${COMMAND} > ./command.sh && unbuffer bash ./command.sh\
          \ | ts\n\n# Push intermediate Docker image for next phase to use\nif [ -z\
          \ \"${BUILD_ONLY}\" ]; then\n  # Augment our output image name with bazel\
          \ to avoid collisions\n  output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-bazel-${CIRCLE_SHA1}\n\
          \  export COMMIT_DOCKER_IMAGE=$output_image\n  docker commit \"$id\" ${COMMIT_DOCKER_IMAGE}\n\
          \  time docker push ${COMMIT_DOCKER_IMAGE}\nfi\n"
        name: Bazel Build
        no_output_timeout: 1h
  pytorch_linux_bazel_test:
    environment: *id014
    machine:
      image: ubuntu-2004:202104-01
    parameters: *id015
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: "set -e\noutput_image=${DOCKER_IMAGE}:${DOCKER_TAG}-bazel-${CIRCLE_SHA1}\n\
          export COMMIT_DOCKER_IMAGE=$output_image\necho \"DOCKER_IMAGE: \"${COMMIT_DOCKER_IMAGE}\n\
          \ntime docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null\n\nif [ -n \"${USE_CUDA_DOCKER_RUNTIME}\"\
          \ ]; then\n  export id=$(docker run --env-file \"${BASH_ENV}\" --cap-add=SYS_PTRACE\
          \ --security-opt seccomp=unconfined --gpus all -t -d -w /var/lib/jenkins\
          \ ${COMMIT_DOCKER_IMAGE})\nelse\n  export id=$(docker run --env-file \"\
          ${BASH_ENV}\" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t\
          \ -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})\nfi\n\nretrieve_test_reports()\
          \ {\n  echo \"retrieving test reports\"\n  docker cp -L $id:/var/lib/jenkins/workspace/bazel-testlogs\
          \ ./ || echo 'No test reports found!'\n}\ntrap \"retrieve_test_reports\"\
          \ ERR\n\nif [[ ${BUILD_ENVIRONMENT} == *\"multigpu\"* ]]; then\n  export\
          \ COMMAND='((echo \"sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/multigpu-test.sh\"\
          ) | docker exec -u jenkins -i \"$id\" bash) 2>&1'\nelse\n  export COMMAND='((echo\
          \ \"sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/test.sh\"\
          ) | docker exec -u jenkins -i \"$id\" bash) 2>&1'\nfi\necho ${COMMAND} >\
          \ ./command.sh && unbuffer bash ./command.sh | ts\n\nretrieve_test_reports\n\
          docker stats --all --no-stream\n"
        name: Test
        no_output_timeout: 90m
    - store_test_results:
        path: bazel-testlogs
  pytorch_linux_build:
    environment: *id014
    machine:
      image: ubuntu-2004:202104-01
    parameters: *id015
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - optional_merge_target_branch
    - setup_ci_environment
    - run:
        command: "set -e\nif [[ ${BUILD_ENVIRONMENT} == *\"pure_torch\"* ]]; then\n\
          \  echo 'BUILD_CAFFE2=OFF' >> \"${BASH_ENV}\"\nfi\nif [[ ${BUILD_ENVIRONMENT}\
          \ == *\"paralleltbb\"* ]]; then\n  echo 'ATEN_THREADING=TBB' >> \"${BASH_ENV}\"\
          \n  echo 'USE_TBB=1' >> \"${BASH_ENV}\"\nelif [[ ${BUILD_ENVIRONMENT} ==\
          \ *\"parallelnative\"* ]]; then\n  echo 'ATEN_THREADING=NATIVE' >> \"${BASH_ENV}\"\
          \nfi\necho \"Parallel backend flags: \"${PARALLEL_FLAGS}\n# Pull Docker\
          \ image and run build\necho \"DOCKER_IMAGE: \"${DOCKER_IMAGE}:${DOCKER_TAG}\n\
          time docker pull ${DOCKER_IMAGE}:${DOCKER_TAG} >/dev/null\nexport id=$(docker\
          \ run --env-file \"${BASH_ENV}\" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined\
          \ --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins\
          \ ${DOCKER_IMAGE}:${DOCKER_TAG})\n\ngit submodule sync && git submodule\
          \ update -q --init --recursive --depth 1\n\ndocker cp /home/circleci/project/.\
          \ $id:/var/lib/jenkins/workspace\n\nexport COMMAND='((echo \"sudo chown\
          \ -R jenkins workspace && export JOB_BASE_NAME=\"$CIRCLE_JOB\" && cd workspace\
          \ && .jenkins/pytorch/build.sh && find ${BUILD_ROOT} -type f -name \"*.a\"\
          \ -or -name \"*.o\" -delete\") | docker exec -u jenkins -i \"$id\" bash)\
          \ 2>&1'\n\necho ${COMMAND} > ./command.sh && unbuffer bash ./command.sh\
          \ | ts\n\n# Copy dist folder back\ndocker cp $id:/var/lib/jenkins/workspace/dist\
          \ /home/circleci/project/. || echo \"Dist folder not found\"\n\n# Push intermediate\
          \ Docker image for next phase to use\nif [ -z \"${BUILD_ONLY}\" ]; then\n\
          \  # Note [Special build images]\n  # The xla build uses the same docker\
          \ image as\n  # pytorch_linux_bionic_py3_6_clang9_build. In the push step,\
          \ we have to\n  # distinguish between them so the test can pick up the correct\
          \ image.\n  output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}\n\
          \  if [[ ${BUILD_ENVIRONMENT} == *\"xla\"* ]]; then\n    export COMMIT_DOCKER_IMAGE=$output_image-xla\n\
          \  elif [[ ${BUILD_ENVIRONMENT} == *\"libtorch\"* ]]; then\n    export COMMIT_DOCKER_IMAGE=$output_image-libtorch\n\
          \  elif [[ ${BUILD_ENVIRONMENT} == *\"paralleltbb\"* ]]; then\n    export\
          \ COMMIT_DOCKER_IMAGE=$output_image-paralleltbb\n  elif [[ ${BUILD_ENVIRONMENT}\
          \ == *\"parallelnative\"* ]]; then\n    export COMMIT_DOCKER_IMAGE=$output_image-parallelnative\n\
          \  elif [[ ${BUILD_ENVIRONMENT} == *\"android-ndk-r19c-x86_64\"* ]]; then\n\
          \    export COMMIT_DOCKER_IMAGE=$output_image-android-x86_64\n  elif [[\
          \ ${BUILD_ENVIRONMENT} == *\"android-ndk-r19c-arm-v7a\"* ]]; then\n    export\
          \ COMMIT_DOCKER_IMAGE=$output_image-android-arm-v7a\n  elif [[ ${BUILD_ENVIRONMENT}\
          \ == *\"android-ndk-r19c-arm-v8a\"* ]]; then\n    export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v8a\n\
          \  elif [[ ${BUILD_ENVIRONMENT} == *\"android-ndk-r19c-x86_32\"* ]]; then\n\
          \    export COMMIT_DOCKER_IMAGE=$output_image-android-x86_32\n  elif [[\
          \ ${BUILD_ENVIRONMENT} == *\"android-ndk-r19c-vulkan-x86_32\"* ]]; then\n\
          \    export COMMIT_DOCKER_IMAGE=$output_image-android-vulkan-x86_32\n  elif\
          \ [[ ${BUILD_ENVIRONMENT} == *\"vulkan-linux\"* ]]; then\n    export COMMIT_DOCKER_IMAGE=$output_image-vulkan\n\
          \  else\n    export COMMIT_DOCKER_IMAGE=$output_image\n  fi\n  docker commit\
          \ \"$id\" ${COMMIT_DOCKER_IMAGE}\n  time docker push ${COMMIT_DOCKER_IMAGE}\n\
          fi\n"
        name: Build
        no_output_timeout: 1h
    - run:
        command: 'cd /pytorch && export COMMIT_TIME=$(git log --max-count=1 --format=%ct
          || echo 0)

          python3 -mpip install requests && \

          SCRIBE_GRAPHQL_ACCESS_TOKEN=${SCRIBE_GRAPHQL_ACCESS_TOKEN} \

          python3 .circleci/scripts/upload_binary_size_to_scuba.py || exit 0

          '
        name: upload build & binary data
        no_output_timeout: 5m
    - store_artifacts:
        path: /home/circleci/project/dist
  pytorch_linux_test:
    environment: *id014
    machine:
      image: ubuntu-2004:202104-01
    parameters: *id015
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: "set -e\nexport PYTHONUNBUFFERED=1\nif [[ \"${DOCKER_IMAGE}\" ==\
          \ *rocm3.9* ]]; then\n  export DOCKER_TAG=\"f3d89a32912f62815e4feaeed47e564e887dffd6\"\
          \nfi\n# See Note [Special build images]\noutput_image=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}\n\
          if [[ ${BUILD_ENVIRONMENT} == *\"xla\"* ]]; then\n  export COMMIT_DOCKER_IMAGE=$output_image-xla\n\
          elif [[ ${BUILD_ENVIRONMENT} == *\"libtorch\"* ]]; then\n  export COMMIT_DOCKER_IMAGE=$output_image-libtorch\n\
          elif [[ ${BUILD_ENVIRONMENT} == *\"paralleltbb\"* ]]; then\n  export COMMIT_DOCKER_IMAGE=$output_image-paralleltbb\n\
          elif [[ ${BUILD_ENVIRONMENT} == *\"parallelnative\"* ]]; then\n  export\
          \ COMMIT_DOCKER_IMAGE=$output_image-parallelnative\nelif [[ ${BUILD_ENVIRONMENT}\
          \ == *\"vulkan-linux\"* ]]; then\n  export COMMIT_DOCKER_IMAGE=$output_image-vulkan\n\
          else\n  export COMMIT_DOCKER_IMAGE=$output_image\nfi\necho \"DOCKER_IMAGE:\
          \ \"${COMMIT_DOCKER_IMAGE}\n\nif [[ ${BUILD_ENVIRONMENT} == *\"paralleltbb\"\
          * ]]; then\n  echo 'ATEN_THREADING=TBB' >> \"${BASH_ENV}\"\n  echo 'USE_TBB=1'\
          \ >> \"${BASH_ENV}\"\nelif [[ ${BUILD_ENVIRONMENT} == *\"parallelnative\"\
          * ]]; then\n  echo 'ATEN_THREADING=NATIVE' >> \"${BASH_ENV}\"\nfi\necho\
          \ \"Parallel backend flags: \"${PARALLEL_FLAGS}\n\ntime docker pull ${COMMIT_DOCKER_IMAGE}\
          \ >/dev/null\n\n# TODO: Make this less painful\nif [ -n \"${USE_CUDA_DOCKER_RUNTIME}\"\
          \ ]; then\n  export id=$(docker run --env-file \"${BASH_ENV}\" --cap-add=SYS_PTRACE\
          \ --security-opt seccomp=unconfined --gpus all --shm-size=2g -t -d -w /var/lib/jenkins\
          \ ${COMMIT_DOCKER_IMAGE})\nelif [[ ${BUILD_ENVIRONMENT} == *\"rocm\"* ]];\
          \ then\n  hostname\n  export id=$(docker run --env-file \"${BASH_ENV}\"\
          \ --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size=8g --ipc=host\
          \ --device /dev/kfd --device /dev/dri --group-add video -t -d -w /var/lib/jenkins\
          \ ${COMMIT_DOCKER_IMAGE})\nelse\n  export id=$(docker run --env-file \"\
          ${BASH_ENV}\" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size=1g\
          \ --ipc=host -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})\nfi\necho\
          \ \"id=${id}\" >> \"${BASH_ENV}\"\n"
        name: Download Docker image
        no_output_timeout: 90m
    - run:
        command: "set -e\nis_vanilla_build() {\n  if [ \"${BUILD_ENVIRONMENT}\" ==\
          \ \"pytorch-linux-bionic-py3.6-clang9-test\" ]; then\n    return 0\n  fi\n\
          \  if [ \"${BUILD_ENVIRONMENT}\" == \"pytorch-linux-xenial-py3.6-gcc5.4-test\"\
          \ ]; then\n    return 0\n  fi\n  return 1\n}\n\nif is_vanilla_build; then\n\
          \  echo \"apt-get update && apt-get install -y qemu-user gdb\" | docker\
          \ exec -u root -i \"$id\" bash\n  echo \"cd workspace/build; qemu-x86_64\
          \ -g 2345 -cpu Broadwell -E ATEN_CPU_CAPABILITY=default ./bin/basic --gtest_filter=BasicTest.BasicTestCPU\
          \ & gdb ./bin/basic -ex 'set pagination off' -ex 'target remote :2345' -ex\
          \ 'continue' -ex 'bt' -ex='set confirm off' -ex 'quit \\$_isvoid(\\$_exitcode)'\"\
          \ | docker exec -u jenkins -i \"$id\" bash\nelse\n  echo \"Skipping for\
          \ ${BUILD_ENVIRONMENT}\"\nfi\n"
        name: Check for no AVX instruction by default
        no_output_timeout: 20m
    - run:
        command: "set -e\n\ncat >docker_commands.sh \\<<EOL\n# ===================\
          \ The following code will be executed inside Docker container ===================\n\
          set -ex\nexport SCRIBE_GRAPHQL_ACCESS_TOKEN=\"${SCRIBE_GRAPHQL_ACCESS_TOKEN}\"\
          \nexport JOB_BASE_NAME=\"$CIRCLE_JOB\"\n${PARALLEL_FLAGS}\ncd workspace\n\
          EOL\nif [[ ${BUILD_ENVIRONMENT} == *\"multigpu\"* ]]; then\n  echo \".jenkins/pytorch/multigpu-test.sh\"\
          \ >> docker_commands.sh\nelif [[ ${BUILD_ENVIRONMENT} == *onnx* ]]; then\n\
          \  echo \"pip install click mock tabulate networkx==2.0\" >> docker_commands.sh\n\
          \  echo \"pip -q install --user \\\"file:///var/lib/jenkins/workspace/third_party/onnx#egg=onnx\\\
          \"\" >> docker_commands.sh\n  echo \".jenkins/caffe2/test.sh\" >> docker_commands.sh\n\
          else\n  echo \".jenkins/pytorch/test.sh\" >> docker_commands.sh\nfi\necho\
          \ \"(cat docker_commands.sh | docker exec -u jenkins -i \"$id\" bash) 2>&1\"\
          \ > command.sh\nunbuffer bash command.sh | ts\n\nif [[ ${BUILD_ENVIRONMENT}\
          \ == *\"coverage\"* ]]; then\n    echo \"Retrieving C++ coverage report\"\
          \n    docker cp $id:/var/lib/jenkins/workspace/build/coverage.info ./test\n\
          fi\nif [[ ${BUILD_ENVIRONMENT} == *\"coverage\"* || ${BUILD_ENVIRONMENT}\
          \ == *\"onnx\"* ]]; then\n    echo \"Retrieving Python coverage report\"\
          \n    docker cp $id:/var/lib/jenkins/workspace/test/.coverage ./test\n \
          \   docker cp $id:/var/lib/jenkins/workspace/test/coverage.xml ./test\n\
          \    python3 -mpip install codecov\n    python3 -mcodecov\nfi\n"
        name: Run tests
        no_output_timeout: 90m
    - run:
        command: 'set -e

          # Retrieving test results should be done as very first step as command never
          fails

          # But is always executed if previous step fails for some reason

          echo "Retrieving test reports"

          docker cp $id:/var/lib/jenkins/workspace/test/test-reports ./ || echo ''No
          test reports found!''

          docker stats --all --no-stream


          cat >docker_commands.sh \<<EOL

          # =================== The following code will be executed inside Docker
          container ===================

          set -ex

          export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}

          export SCRIBE_GRAPHQL_ACCESS_TOKEN="${SCRIBE_GRAPHQL_ACCESS_TOKEN}"

          export CIRCLE_TAG="${CIRCLE_TAG:-}"

          export CIRCLE_SHA1="$CIRCLE_SHA1"

          export CIRCLE_PR_NUMBER="${CIRCLE_PR_NUMBER:-}"

          export CIRCLE_BRANCH="$CIRCLE_BRANCH"

          export JOB_BASE_NAME="$CIRCLE_JOB"

          export CIRCLE_WORKFLOW_ID="$CIRCLE_WORKFLOW_ID"

          cd workspace

          export PYTHONPATH="\${PWD}"

          python tools/stats/print_test_stats.py --upload-to-s3 --compare-with-s3
          test

          EOL

          echo "(cat docker_commands.sh | docker exec -u jenkins -e LANG=C.UTF-8 -i
          "$id" bash) 2>&1" > command.sh

          unbuffer bash command.sh | ts

          '
        name: Report results
        no_output_timeout: 5m
        when: always
    - store_test_results:
        path: test-reports
    - store_artifacts:
        path: test/.coverage
    - store_artifacts:
        path: test/coverage.xml
  pytorch_macos_10_13_py3_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-build
    macos:
      xcode: '12.0'
    steps:
    - checkout
    - run_brew_for_macos_build
    - run:
        command: 'set -e

          export IN_CI=1


          # Install sccache

          sudo curl --retry 3 https://s3.amazonaws.com/ossci-macos/sccache_v2.15 --output
          /usr/local/bin/sccache

          sudo chmod +x /usr/local/bin/sccache

          export SCCACHE_BUCKET=ossci-compiler-cache-circleci-v2


          # This IAM user allows write access to S3 bucket for sccache

          set +x

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_S3_BUCKET_V4}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET_V4}

          set -x


          chmod a+x .jenkins/pytorch/macos-build.sh

          unbuffer .jenkins/pytorch/macos-build.sh 2>&1 | ts

          '
        name: Build
        no_output_timeout: 1h
    - persist_to_workspace:
        paths:
        - miniconda3
        root: /Users/distiller/workspace/
  pytorch_macos_10_13_py3_lite_interpreter_build_test:
    environment:
      BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-test
    macos:
      xcode: '12.0'
    steps:
    - checkout
    - attach_workspace:
        at: ~/workspace
    - run_brew_for_macos_build
    - run:
        command: 'set -e

          export IN_CI=1

          export BUILD_LITE_INTERPRETER=1

          chmod a+x ${HOME}/project/.jenkins/pytorch/macos-lite-interpreter-build-test.sh

          unbuffer ${HOME}/project/.jenkins/pytorch/macos-lite-interpreter-build-test.sh
          2>&1 | ts

          '
        name: Test
        no_output_timeout: 1h
    - store_test_results:
        path: test/test-reports
  pytorch_macos_10_13_py3_test:
    environment:
      BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-test
    macos:
      xcode: '12.0'
    steps:
    - checkout
    - attach_workspace:
        at: ~/workspace
    - run_brew_for_macos_build
    - run:
        command: 'set -e

          export IN_CI=1


          chmod a+x .jenkins/pytorch/macos-test.sh

          unbuffer .jenkins/pytorch/macos-test.sh 2>&1 | ts

          '
        name: Test
        no_output_timeout: 1h
    - run:
        command: 'set -ex

          source /Users/distiller/workspace/miniconda3/bin/activate

          pip install boto3

          export PYTHONPATH="$PWD"


          # Using the same IAM user to write stats to our OSS bucket

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_S3_BUCKET_V4}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET_V4}

          python tools/stats/print_test_stats.py --upload-to-s3 --compare-with-s3
          test

          '
        name: Report results
        no_output_timeout: 5m
        when: always
    - store_test_results:
        path: test/test-reports
  pytorch_macos_10_15_py3_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-macos-10.15-py3-arm64-build
    macos:
      xcode: 12.3.0
    steps:
    - checkout
    - run_brew_for_macos_build
    - run:
        command: 'set -e

          export IN_CI=1

          export CROSS_COMPILE_ARM64=1


          # Install sccache

          sudo curl --retry 3 https://s3.amazonaws.com/ossci-macos/sccache_v2.15 --output
          /usr/local/bin/sccache

          sudo chmod +x /usr/local/bin/sccache

          export SCCACHE_BUCKET=ossci-compiler-cache-circleci-v2


          # This IAM user allows write access to S3 bucket for sccache

          set +x

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_S3_BUCKET_V4}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET_V4}

          set -x


          chmod a+x .jenkins/pytorch/macos-build.sh

          unbuffer .jenkins/pytorch/macos-build.sh 2>&1 | ts

          '
        name: Build
        no_output_timeout: 1h
    - persist_to_workspace:
        paths:
        - miniconda3
        root: /Users/distiller/workspace/
    - store_artifacts:
        path: /Users/distiller/project/dist
  pytorch_python_doc_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-python-doc-push
      DOCKER_IMAGE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
    machine:
      image: ubuntu-2004:202104-01
    resource_class: large
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: 'set -ex

          export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}

          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}

          tag=${CIRCLE_TAG:1:5}

          target=${tag:-master}

          echo "building for ${target}"

          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null

          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt
          seccomp=unconfined -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})


          export COMMAND=''((echo "sudo chown -R jenkins workspace && cd workspace
          && ''"export CIRCLE_SHA1=''$CIRCLE_SHA1''"'' && . ./.circleci/scripts/python_doc_push_script.sh
          docs/''$target'' ''$target'' site") | docker exec -u jenkins -i "$id" bash)
          2>&1''


          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts


          mkdir -p ~/workspace/build_artifacts

          docker cp $id:/var/lib/jenkins/workspace/pytorch.github.io/docs/master ~/workspace/build_artifacts

          docker cp $id:/var/lib/jenkins/workspace/pytorch.github.io /tmp/workspace


          # Save the docs build so we can debug any problems

          export DEBUG_COMMIT_DOCKER_IMAGE=${COMMIT_DOCKER_IMAGE}-debug

          docker commit "$id" ${DEBUG_COMMIT_DOCKER_IMAGE}

          time docker push ${DEBUG_COMMIT_DOCKER_IMAGE}

          '
        name: Doc Build and Push
        no_output_timeout: 1h
    - persist_to_workspace:
        paths:
        - .
        root: /tmp/workspace
    - store_artifacts:
        destination: docs
        path: ~/workspace/build_artifacts/master
  pytorch_windows_build:
    environment: &id016
      BUILD_ENVIRONMENT: <<parameters.build_environment>>
      CUDA_VERSION: <<parameters.cuda_version>>
      JOB_BASE_NAME: <<parameters.test_name>>
      JOB_EXECUTOR: <<parameters.executor>>
      PYTHON_VERSION: <<parameters.python_version>>
      SCCACHE_BUCKET: ossci-compiler-cache
      TORCH_CUDA_ARCH_LIST: 5.2 7.5
      USE_CUDA: <<parameters.use_cuda>>
      VC_PRODUCT: <<parameters.vc_product>>
      VC_VERSION: <<parameters.vc_version>>
      VC_YEAR: <<parameters.vc_year>>
    executor: <<parameters.executor>>
    parameters:
      build_environment:
        default: ''
        type: string
      cuda_version:
        default: '10.1'
        type: string
      executor:
        default: windows-xlarge-cpu-with-nvidia-cuda
        type: string
      python_version:
        default: '3.8'
        type: string
      test_name:
        default: ''
        type: string
      use_cuda:
        default: ''
        type: string
      vc_product:
        default: BuildTools
        type: string
      vc_version:
        default: '14.16'
        type: string
      vc_year:
        default: '2019'
        type: string
    steps:
    - checkout
    - run:
        command: 'powershell .circleci/scripts/vs_install.ps1

          '
        name: Install VS2019 toolchain
        no_output_timeout: 10m
    - run:
        command: "if [[ \"${USE_CUDA}\" == \"1\" ]]; then\n  .circleci/scripts/windows_cuda_install.sh\n\
          fi\n"
        name: Install Cuda
        no_output_timeout: 30m
    - run:
        command: "if [[ \"${USE_CUDA}\" == \"1\" ]]; then\n  .circleci/scripts/windows_cudnn_install.sh\n\
          fi\n"
        name: Install Cudnn
    - run:
        command: 'set -e

          set +x

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_WIN_BUILD_V1}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_WIN_BUILD_V1}

          set -x

          .jenkins/pytorch/win-build.sh

          '
        name: Build
        no_output_timeout: 90m
    - persist_to_workspace:
        paths: build-results
        root: C:/w
    - store_artifacts:
        path: C:/w/build-results
  pytorch_windows_test:
    environment: *id016
    executor: <<parameters.executor>>
    parameters:
      build_environment:
        default: ''
        type: string
      cuda_version:
        default: '10.1'
        type: string
      executor:
        default: windows-medium-cpu-with-nvidia-cuda
        type: string
      python_version:
        default: '3.8'
        type: string
      test_name:
        default: ''
        type: string
      use_cuda:
        default: ''
        type: string
      vc_product:
        default: BuildTools
        type: string
      vc_version:
        default: '14.16'
        type: string
      vc_year:
        default: '2019'
        type: string
    steps:
    - checkout
    - attach_workspace:
        at: c:/users/circleci/workspace
    - run:
        command: 'powershell .circleci/scripts/vs_install.ps1

          '
        name: Install VS2019 toolchain
        no_output_timeout: 10m
    - run:
        command: "if [[ \"${CUDA_VERSION}\" != \"cpu\" ]]; then\n  if [[ \"${CUDA_VERSION}\"\
          \ != \"10\" || \"${JOB_EXECUTOR}\" != \"windows-with-nvidia-gpu\" ]]; then\n\
          \    .circleci/scripts/windows_cuda_install.sh\n  fi\nfi\n"
        name: Install Cuda
        no_output_timeout: 30m
    - run:
        command: "if [[ \"${CUDA_VERSION}\" != \"cpu\" ]]; then\n  .circleci/scripts/windows_cudnn_install.sh\n\
          fi\n"
        name: Install Cudnn
    - run:
        command: 'set -e

          export IN_CI=1

          set +x

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_WIN_BUILD_V1}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_WIN_BUILD_V1}

          set -x

          .jenkins/pytorch/win-test.sh

          '
        name: Test
        no_output_timeout: 30m
    - run:
        command: 'set -ex

          export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_WIN_BUILD_V1}

          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_WIN_BUILD_V1}

          export PYTHONPATH="$PWD"

          pip install typing_extensions boto3

          python tools/stats/print_test_stats.py --upload-to-s3 --compare-with-s3
          test

          '
        name: Report results
        no_output_timeout: 5m
        when: always
    - store_test_results:
        path: test/test-reports
    - store_artifacts:
        path: test/coverage.xml
  pytorch_windows_test_multigpu:
    machine:
      image: ubuntu-2004:202104-01
    steps:
    - checkout
    - run:
        command: 'set -e

          python3 -m pip install requests

          python3 ./.circleci/scripts/trigger_azure_pipeline.py

          '
        name: Test
        no_output_timeout: 90m
  smoke_linux_test:
    environment: *id006
    machine:
      image: ubuntu-2004:202104-01
    parameters: *id007
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - run:
        command: 'set -ex

          cat >/home/circleci/project/ci_test_script.sh \<<EOL

          # The following code will be executed inside Docker container

          set -eux -o pipefail

          /builder/smoke_test.sh

          # The above code will be executed inside Docker container

          EOL

          '
        name: Test
        no_output_timeout: 1h
    - run:
        command: .circleci/scripts/binary_run_in_docker.sh
        name: Run in docker
  smoke_mac_test:
    environment: *id006
    macos:
      xcode: '12.0'
    parameters: *id007
    resource_class: << parameters.resource_class >>
    steps:
    - checkout
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - brew_update
    - run:
        command: .circleci/scripts/binary_install_miniconda.sh
        name: Install miniconda
        no_output_timeout: 1h
    - run:
        command: 'set -ex

          source "/Users/distiller/project/env"

          export "PATH=$workdir/miniconda/bin:$PATH"

          # TODO unbuffer and ts this, but it breaks cause miniconda overwrites

          # tclsh. But unbuffer and ts aren''t that important so they''re just

          # disabled for now

          ./builder/smoke_test.sh

          '
        name: Build
        no_output_timeout: 1h
  smoke_windows_test:
    environment: *id010
    executor: <<parameters.executor>>
    parameters:
      build_environment:
        default: ''
        type: string
      executor:
        default: windows-medium-cpu-with-nvidia-cuda
        type: string
    steps:
    - checkout
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: .circleci/scripts/binary_populate_env.sh
        name: Set up binary env variables
    - run:
        command: 'set -eux -o pipefail

          export TEST_NIGHTLY_PACKAGE=1

          script="/c/w/p/.circleci/scripts/binary_windows_test.sh"

          cat "$script"

          source "$script"

          '
        name: Test
        no_output_timeout: 1h
  update_s3_htmls:
    machine:
      image: ubuntu-2004:202104-01
    resource_class: medium
    steps:
    - checkout
    - setup_linux_system_environment
    - run:
        command: .circleci/scripts/binary_checkout.sh
        name: Checkout pytorch/builder repo
    - run:
        command: "our_upload_folder=nightly/\n# On tags upload to test instead\nif\
          \ [[ -n \"${CIRCLE_TAG}\" ]]; then\n  our_upload_folder=test/\nfi\necho\
          \ \"export PIP_UPLOAD_FOLDER=${our_upload_folder}\" >> ${BASH_ENV}\n"
        name: define PIP_UPLOAD_FOLDER
    - run:
        command: "set +x\necho \"declare -x \\\"AWS_ACCESS_KEY_ID=${PYTORCH_BINARY_AWS_ACCESS_KEY_ID}\\\
          \"\" >> /home/circleci/project/env\necho \"declare -x \\\"AWS_SECRET_ACCESS_KEY=${PYTORCH_BINARY_AWS_SECRET_ACCESS_KEY}\\\
          \"\" >> /home/circleci/project/env\nsource /home/circleci/project/env\n\
          set -eux -o pipefail\nretry () {\n    $*  || (sleep 1 && $*) || (sleep 2\
          \ && $*) || (sleep 4 && $*) || (sleep 8 && $*)\n}\nretry pip install awscli==1.6\n\
          \"/home/circleci/project/builder/cron/update_s3_htmls.sh\"\n"
        name: Update s3 htmls
        no_output_timeout: 1h
parameters:
  run_binary_tests:
    default: false
    type: boolean
  run_build:
    default: true
    type: boolean
  run_master_build:
    default: false
    type: boolean
  run_slow_gradcheck_build:
    default: false
    type: boolean
promote_common:
  docker: *id011
  environment: *id012
  parameters: *id013
pytorch_android_params:
  environment: *id017
  parameters: *id018
pytorch_ios_params:
  environment: *id001
  parameters: *id002
pytorch_params:
  environment: *id014
  parameters: *id015
  resource_class: << parameters.resource_class >>
pytorch_windows_params:
  environment: *id016
  parameters:
    build_environment:
      default: ''
      type: string
    cuda_version:
      default: '10.1'
      type: string
    executor:
      default: windows-xlarge-cpu-with-nvidia-cuda
      type: string
    python_version:
      default: '3.8'
      type: string
    test_name:
      default: ''
      type: string
    use_cuda:
      default: ''
      type: string
    vc_product:
      default: BuildTools
      type: string
    vc_version:
      default: '14.16'
      type: string
    vc_year:
      default: '2019'
      type: string
version: 2.1
workflows: {}
